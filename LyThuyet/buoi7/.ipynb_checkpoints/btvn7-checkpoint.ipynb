{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# H·ªç v√† T√™n: L√¢m Quang Ph√∫\n",
    "# MSSV: 21094601"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# C√¢u 1: C√†i ƒë·∫∑t gi·∫£i thu·∫≠t Viterbi v·ªõi framcode: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<a href=\"https://imgbb.com/\"><img src=\"https://i.ibb.co/hcrtb86/image.png\" alt=\"image\" border=\"0\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = ('normal', 'cold', 'dizzy')\n",
    "states = ('Healthy', 'Fever')\n",
    "start_p = {'Healthy': 0.6, 'Fever': 0.4}\n",
    "trans_p = {\n",
    " 'Healthy' : {'Healthy': 0.7, 'Fever': 0.3},\n",
    " 'Fever' : {'Healthy': 0.4, 'Fever': 0.6}\n",
    " }\n",
    "emit_p = {\n",
    " 'Healthy' : {'normal': 0.5, 'cold': 0.4, 'dizzy': 0.1},\n",
    " 'Fever' : {'normal': 0.1, 'cold': 0.3, 'dizzy': 0.6}\n",
    " }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "obs: Danh s√°ch c√°c quan s√°t ƒë∆∞·ª£c th·ª±c hi·ªán trong qu√° tr√¨nh quan s√°t. \n",
    "\n",
    "states: T·∫≠p h·ª£p c√°c tr·∫°ng th√°i c√≥ th·ªÉ trong m√¥ h√¨nh. \n",
    "\n",
    "start_p: X√°c su·∫•t c·ªßa vi·ªác b·∫Øt ƒë·∫ßu t·ª´ m·ªói tr·∫°ng th√°i. \n",
    "\n",
    "trans_p: X√°c su·∫•t chuy·ªÉn t·ª´ m·ªói tr·∫°ng th√°i sang tr·∫°ng th√°i kh√°c. \n",
    "\n",
    "emit_p: X√°c su·∫•t c·ªßa vi·ªác quan s√°t m·ªôt gi√° tr·ªã nh·∫•t ƒë·ªãnh t·ª´ m·ªói tr·∫°ng th√°i.\n",
    "'''\n",
    "\n",
    "def viterbi(obs, states, start_p, trans_p, emit_p):\n",
    "    V = [{}]\n",
    "    for st in states:\n",
    "        V[0][st] = {\"prob\": start_p[st] * emit_p[st][obs[0]], \"prev\": None}\n",
    "    for t in range(1, len(obs)):\n",
    "        V.append({})\n",
    "        for st in states:\n",
    "            max_tr_prob = max(V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st] for prev_st in states)\n",
    "            for prev_st in states:\n",
    "                if V[t-1][prev_st][\"prob\"]*trans_p[prev_st][st] == max_tr_prob:\n",
    "                    max_prob = max_tr_prob * emit_p[st][obs[t]]\n",
    "                    V[t][st] = {\"prob\": max_prob, \"prev\": prev_st}\n",
    "                    break\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'Healthy': {'prob': 0.3, 'prev': None},\n",
       "  'Fever': {'prob': 0.04000000000000001, 'prev': None}},\n",
       " {'Healthy': {'prob': 0.084, 'prev': 'Healthy'},\n",
       "  'Fever': {'prob': 0.027, 'prev': 'Healthy'}},\n",
       " {'Healthy': {'prob': 0.00588, 'prev': 'Healthy'},\n",
       "  'Fever': {'prob': 0.01512, 'prev': 'Healthy'}}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "V = viterbi(obs, states, start_p, trans_p, emit_p)\n",
    "V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# B√†i 2: C√†i ƒë·∫∑t gi·∫£i thu·∫≠t BFR v√† ·ª©ng d·ª•ng gi·∫£i thu·∫≠t n√†y cho 1 b·ªô d·ªØ li·ªáu "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ B∆∞·ªõc 1. T·∫£i 20% d·ªØ li·ªáu m·ªôt c√°ch ng·∫´u nhi√™n.  \n",
    "+ B∆∞·ªõc 2. Ch·∫°y thu·∫≠t to√°n K-Means (v√≠ d·ª•: t·ª´ th∆∞ vi·ªán sklearn) v·ªõi m·ªôt K l·ªõn (v√≠ d·ª•: 5 l·∫ßn s·ªë l∆∞·ª£ng c·ª•m d·ªØ li·ªáu ƒë·∫ßu v√†o) tr√™n d·ªØ li·ªáu trong b·ªô nh·ªõ s·ª≠ d·ª•ng kho·∫£ng c√°ch Euclidean l√†m ƒë·ªô ƒëo t∆∞∆°ng ƒë·ªìng.  \n",
    "+ B∆∞·ªõc 3. Trong k·∫øt qu·∫£ c·ªßa K-Means t·ª´ B∆∞·ªõc 2, di chuy·ªÉn t·∫•t c·∫£ c√°c c·ª•m ch·ªâ ch·ª©a m·ªôt ƒëi·ªÉm v√†o RS (c√°c ƒëi·ªÉm ngo·∫°i lai).  \n",
    "+ B∆∞·ªõc 4. Ch·∫°y l·∫°i thu·∫≠t to√°n K-Means ƒë·ªÉ ph√¢n c·ª•m ph·∫ßn c√≤n l·∫°i c·ªßa c√°c ƒëi·ªÉm d·ªØ li·ªáu v·ªõi K = s·ªë l∆∞·ª£ng c·ª•m d·ªØ li·ªáu ƒë·∫ßu v√†o.  \n",
    "+ B∆∞·ªõc 5. S·ª≠ d·ª•ng k·∫øt qu·∫£ c·ªßa K-Means t·ª´ B∆∞·ªõc 4 ƒë·ªÉ t·∫°o ra c√°c c·ª•m DS (t·ª©c l√† lo·∫°i b·ªè c√°c ƒëi·ªÉm v√† t·∫°o th·ªëng k√™). Qu√° tr√¨nh kh·ªüi t·∫°o c·ªßa DS ƒë√£ ho√†n th√†nh, cho ƒë·∫øn nay, b·∫°n c√≥ K s·ªë l∆∞·ª£ng c√°c c·ª•m DS (t·ª´ B∆∞·ªõc 5) v√† m·ªôt s·ªë l∆∞·ª£ng c√°c c·ª•m RS (t·ª´ B∆∞·ªõc 3).  \n",
    "+ B∆∞·ªõc 6. Ch·∫°y thu·∫≠t to√°n K-Means tr√™n c√°c ƒëi·ªÉm trong RS v·ªõi m·ªôt K l·ªõn (v√≠ d·ª•: 5 l·∫ßn s·ªë l∆∞·ª£ng c·ª•m d·ªØ li·ªáu ƒë·∫ßu v√†o) ƒë·ªÉ t·∫°o ra CS (c√°c c·ª•m c√≥ nhi·ªÅu h∆°n m·ªôt ƒëi·ªÉm) v√† RS (c√°c c·ª•m ch·ªâ c√≥ m·ªôt ƒëi·ªÉm).  \n",
    "+ B∆∞·ªõc 7. T·∫£i 20% d·ªØ li·ªáu m·ªôt c√°ch ng·∫´u nhi√™n.  \n",
    "+ B∆∞·ªõc 8. ƒê·ªëi v·ªõi c√°c ƒëi·ªÉm m·ªõi, so s√°nh ch√∫ng v·ªõi m·ªói c·ª•m DS b·∫±ng kho·∫£ng c√°ch Mahalanobis v√† g√°n ch√∫ng v√†o c·ª•m DS g·∫ßn nh·∫•t n·∫øu kho·∫£ng c√°ch < 2 .ùëë  \n",
    "+ B∆∞·ªõc 9. ƒê·ªëi v·ªõi c√°c ƒëi·ªÉm m·ªõi kh√¥ng ƒë∆∞·ª£c g√°n v√†o c√°c c·ª•m DS, s·ª≠ d·ª•ng kho·∫£ng c√°ch Mahalanobis v√† g√°n c√°c ƒëi·ªÉm v√†o c√°c c·ª•m CS g·∫ßn nh·∫•t n·∫øu kho·∫£ng c√°ch < 2 .ùëë  \n",
    "+ B∆∞·ªõc 10. ƒê·ªëi v·ªõi c√°c ƒëi·ªÉm m·ªõi kh√¥ng ƒë∆∞·ª£c g√°n v√†o m·ªôt c·ª•m DS ho·∫∑c m·ªôt c·ª•m CS, g√°n ch√∫ng v√†o RS.  \n",
    "+ B∆∞·ªõc 11. Ch·∫°y thu·∫≠t to√°n K-Means tr√™n RS v·ªõi m·ªôt K l·ªõn (v√≠ d·ª•: 5 l·∫ßn s·ªë l∆∞·ª£ng c·ª•m d·ªØ li·ªáu ƒë·∫ßu v√†o) ƒë·ªÉ t·∫°o ra CS (c√°c c·ª•m c√≥ nhi·ªÅu h∆°n m·ªôt ƒëi·ªÉm) v√† RS (c√°c c·ª•m ch·ªâ c√≥ m·ªôt ƒëi·ªÉm).  \n",
    "+ B∆∞·ªõc 12. H·ª£p nh·∫•t c√°c c·ª•m CS c√≥ kho·∫£ng c√°ch Mahalanobis < 2 .ùëë  \n",
    "L·∫∑p l·∫°i c√°c B∆∞·ªõc 7 - 12. N·∫øu ƒë√¢y l√† l·∫ßn ch·∫°y cu·ªëi c√πng (sau kh·ªëi d·ªØ li·ªáu cu·ªëi c√πng), h·ª£p nh·∫•t c√°c c·ª•m CS v·ªõi c√°c c·ª•m DS c√≥ kho·∫£ng c√°ch Mahalanobis < 2 .ùëë. ‚óè T·∫°i m·ªói l·∫ßn ch·∫°y, bao g·ªìm c·∫£ b∆∞·ªõc kh·ªüi t·∫°o, b·∫°n c·∫ßn ƒë·∫øm v√† ƒë·∫ßu ra s·ªë l∆∞·ª£ng c√°c ƒëi·ªÉm lo·∫°i b·ªè, s·ªë l∆∞·ª£ng c√°c c·ª•m trong CS, s·ªë l∆∞·ª£ng c√°c ƒëi·ªÉm n√©n, v√† s·ªë l∆∞·ª£ng c√°c ƒëi·ªÉm trong t·∫≠p gi·ªØ l·∫°i."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.cluster import KMeans\n",
    "import time\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data shape: (150, 4)\n",
      "Target shape: (150,)\n",
      "Feature names: ['sepal length (cm)', 'sepal width (cm)', 'petal length (cm)', 'petal width (cm)']\n",
      "Target names: ['setosa' 'versicolor' 'virginica']\n"
     ]
    }
   ],
   "source": [
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "\n",
    "# M√¥ t·∫£ d·ªØ li·ªáu tr·∫£ v·ªÅ\n",
    "print(\"Data shape:\", iris.data.shape)\n",
    "print(\"Target shape:\", iris.target.shape)\n",
    "print(\"Feature names:\", iris.feature_names)\n",
    "print(\"Target names:\", iris.target_names)\n",
    "# print(\"Description:\", iris.DESCR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_cluster = 3\n",
    "features = iris.data.astype(int)\n",
    "index = np.arange(len(features))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DS Cluster ID: 0, Number of Points: 8\n",
      "DS Cluster ID: 1, Number of Points: 14\n",
      "DS Cluster ID: 2, Number of Points: 2\n",
      "Number of Points in RS: 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def mahalanobis_distance(x, mean, cov):\n",
    "    x_minus_mean = x - mean\n",
    "    return np.sqrt(np.dot(np.dot(x_minus_mean, np.linalg.inv(cov)), x_minus_mean.T))\n",
    "\n",
    "def initialize_DS(initial_data, initial_index, num_clusters):\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    clusters = kmeans.fit_predict(initial_data)\n",
    "    unique_clusters, counts = np.unique(clusters, return_counts=True)\n",
    "    RS_list = unique_clusters[counts == 1]\n",
    "    RS_index = initial_index[np.isin(clusters, RS_list)]\n",
    "    remaining_index = initial_index[~np.isin(clusters, RS_list)]\n",
    "    remaining_data = initial_data[~np.isin(clusters, RS_list)]\n",
    "    \n",
    "    kmeans2 = KMeans(n_clusters=num_clusters)\n",
    "    clusters2 = kmeans2.fit_predict(remaining_data)\n",
    "    \n",
    "    DS = {}\n",
    "    for cluster_id in np.unique(clusters2):\n",
    "        point_indices = remaining_index[clusters2 == cluster_id]\n",
    "        points = remaining_data[clusters2 == cluster_id]\n",
    "        if len(point_indices) > 0:  # Ki·ªÉm tra ch·ªâ m·ª•c tr∆∞·ªõc khi s·ª≠ d·ª•ng\n",
    "            DS[cluster_id] = {\n",
    "                'N': point_indices.tolist(),\n",
    "                'SUM': np.sum(points, axis=0),\n",
    "                'SUMSQ': np.sum(points ** 2, axis=0)\n",
    "            }\n",
    "    return DS, RS_index\n",
    "def initialize_CS(RS_index, initial_data, initial_index, num_clusters):\n",
    "    if len(RS_index) == 0:  # Ki·ªÉm tra n·∫øu RS_index l√† tr·ªëng\n",
    "        return {}  # Tr·∫£ v·ªÅ m·ªôt t·ª´ ƒëi·ªÉn tr·ªëng\n",
    "\n",
    "    valid_RS_index = [idx for idx in RS_index if idx < len(initial_data)]  # L·ªçc ch·ªâ m·ª•c h·ª£p l·ªá\n",
    "    if len(valid_RS_index) == 0:  # Ki·ªÉm tra n·∫øu kh√¥ng c√≥ ch·ªâ m·ª•c h·ª£p l·ªá\n",
    "        return {}  # Tr·∫£ v·ªÅ m·ªôt t·ª´ ƒëi·ªÉn tr·ªëng\n",
    "\n",
    "    kmeans = KMeans(n_clusters=num_clusters)\n",
    "    clusters = kmeans.fit_predict(initial_data[valid_RS_index])\n",
    "    CS_dict = {}\n",
    "    for cluster_label in np.unique(clusters):\n",
    "        cluster_indices = initial_index[valid_RS_index][clusters == cluster_label]\n",
    "        cluster_feature = initial_data[valid_RS_index][clusters == cluster_label]\n",
    "        if len(cluster_indices) > 1:\n",
    "            CS_dict[cluster_label] = {\n",
    "                'N': cluster_indices.tolist(),\n",
    "                'SUM': np.sum(cluster_feature, axis=0),\n",
    "                'SUMSQ': np.sum(cluster_feature ** 2, axis=0)\n",
    "            }\n",
    "    return CS_dict\n",
    "\n",
    "\n",
    "\n",
    "def merge_CS(CS_dict, threshold):\n",
    "    merged_clusters = set()\n",
    "    for cluster1_id, cluster1_stats in CS_dict.items():\n",
    "        for cluster2_id, cluster2_stats in CS_dict.items():\n",
    "            if cluster1_id != cluster2_id and cluster2_id not in merged_clusters:\n",
    "                centroid_1 = cluster1_stats['SUM'] / len(cluster1_stats['N'])\n",
    "                centroid_2 = cluster2_stats['SUM'] / len(cluster2_stats['N'])\n",
    "                var1 = (cluster1_stats['SUMSQ'] / len(cluster1_stats['N'])) - (cluster1_stats['SUM'] / len(cluster1_stats['N']))**2\n",
    "                var2 = (cluster2_stats['SUMSQ'] / len(cluster2_stats['N'])) - (cluster2_stats['SUM'] / len(cluster2_stats['N']))**2\n",
    "                m1 = (centroid_1 - centroid_2) / var1\n",
    "                m2 = (centroid_1 - centroid_2) / var2\n",
    "                md1 = np.dot(m1, m1) ** (1/2)\n",
    "                md2 = np.dot(m2, m2) ** (1/2)\n",
    "                md = min(md1,md2)\n",
    "                if md < threshold:\n",
    "                    merged_N = cluster1_stats['N'] + cluster2_stats['N']\n",
    "                    merged_SUM = cluster1_stats['SUM'] + cluster2_stats['SUM']\n",
    "                    merged_SUMSQ = cluster1_stats['SUMSQ'] + cluster2_stats['SUMSQ']\n",
    "                    CS_dict[cluster1_id] = {\n",
    "                        'N': merged_N,\n",
    "                        'SUM': merged_SUM,\n",
    "                        'SUMSQ': merged_SUMSQ\n",
    "                    }\n",
    "                    merged_clusters.add(cluster2_id)\n",
    "    for cluster_id in merged_clusters:\n",
    "        del CS_dict[cluster_id]\n",
    "    return CS_dict\n",
    "\n",
    "# Load iris dataset\n",
    "iris = load_iris()\n",
    "features = iris.data\n",
    "index = np.arange(len(features))\n",
    "\n",
    "# Initialize variables\n",
    "num_clusters = 3\n",
    "threshold = 2 * np.sqrt(features.shape[1])  # 2 * sqrt(dimensionality)\n",
    "\n",
    "DS_dict = {}\n",
    "RS_indices = []\n",
    "\n",
    "for chunk_num in range(5):\n",
    "    process_index = index[chunk_num * len(index)//5 : (chunk_num+1) * len(index)//5]\n",
    "    process_data = features[chunk_num * len(index)//5 : (chunk_num+1) * len(index)//5]\n",
    "\n",
    "    # Step 1: Load 20% data randomly\n",
    "    initial_data = process_data\n",
    "    initial_index = process_index\n",
    "    \n",
    "    # Step 2 & 3: Run K-Means and move outliers to RS\n",
    "    num_clusters_step2 = min(num_clusters * 5, len(initial_data))  # Ensure enough samples for clustering\n",
    "    DS, RS_index = initialize_DS(initial_data, initial_index, num_clusters_step2)\n",
    "    \n",
    "    # Step 4: K-Means on rest points\n",
    "    remaining_index = np.setdiff1d(initial_index, RS_index)\n",
    "    remaining_data = features[remaining_index]\n",
    "    num_clusters_step4 = min(num_clusters, len(remaining_data))  # Ensure enough samples for clustering\n",
    "    kmeans4 = KMeans(n_clusters=num_clusters_step4)\n",
    "    clusters4 = kmeans4.fit_predict(remaining_data)\n",
    "\n",
    "    # Step 5: Generate DS\n",
    "    for cluster_id in np.unique(clusters4):\n",
    "        point_indices = remaining_index[clusters4 == cluster_id]\n",
    "        points = remaining_data[clusters4 == cluster_id]\n",
    "        DS_dict[cluster_id] = {\n",
    "            'N': point_indices.tolist(),\n",
    "            'SUM': np.sum(points, axis=0),\n",
    "            'SUMSQ': np.sum(points ** 2, axis=0)\n",
    "        }\n",
    "    \n",
    "    # Step 6: K-Means on RS\n",
    "    RS_features = initial_data[np.isin(initial_index, RS_index)]\n",
    "    num_clusters_step6 = min(num_clusters * 5, len(RS_features))  # Ensure enough samples for clustering\n",
    "    CS_dict = initialize_CS(RS_index, initial_data, initial_index, num_clusters_step6)\n",
    "    CS_dict = merge_CS(CS_dict, threshold)\n",
    "    \n",
    "    # Last chunk of data\n",
    "    if chunk_num == 4:\n",
    "        # Merge CS with DS\n",
    "        for cs_cluster_id, cs_stats in CS_dict.items():\n",
    "            for ds_cluster_id, ds_stats in DS_dict.items():\n",
    "                centroid_1 = cs_stats['SUM'] / len(cs_stats['N'])\n",
    "                centroid_2 = ds_stats['SUM'] / len(ds_stats['N'])\n",
    "                var1 = (cs_stats['SUMSQ'] / len(cs_stats['N'])) - (cs_stats['SUM'] / len(cs_stats['N']))**2\n",
    "                var2 = (ds_stats['SUMSQ'] / len(ds_stats['N'])) - (ds_stats['SUM'] / len(ds_stats['N']))**2\n",
    "                m1 = (centroid_1 - centroid_2) / var1\n",
    "                m2 = (centroid_1 - centroid_2) / var2\n",
    "                md1 = np.dot(m1, m1) ** (1/2)\n",
    "                md2 = np.dot(m2, m2) ** (1/2)\n",
    "                md = min(md1,md2)\n",
    "                if md < threshold:\n",
    "                    merged_indices = cs_stats['N'] + ds_stats['N']\n",
    "                    merged_SUM = cs_stats['SUM'] + ds_stats['SUM']\n",
    "                    merged_SUMSQ = cs_stats['SUMSQ'] + ds_stats['SUMSQ']\n",
    "                    DS_dict[ds_cluster_id] = {\n",
    "                        'N': merged_indices,\n",
    "                        'SUM': merged_SUM,\n",
    "                        'SUMSQ': merged_SUMSQ\n",
    "                    }\n",
    "                    del CS_dict[cs_cluster_id]\n",
    "                    break  # Break to avoid modifying CS_dict while iterating\n",
    "\n",
    "# Write the results\n",
    "for cluster_id, cluster_info in DS_dict.items():\n",
    "    print(f\"DS Cluster ID: {cluster_id}, Number of Points: {len(cluster_info['N'])}\")\n",
    "\n",
    "for cluster_id, cluster_info in CS_dict.items():\n",
    "    print(f\"CS Cluster ID: {cluster_id}, Number of Points: {len(cluster_info['N'])}\")\n",
    "\n",
    "print(f\"Number of Points in RS: {len(RS_index)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
